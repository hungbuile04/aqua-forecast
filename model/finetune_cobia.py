import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
import warnings
import os
from pathlib import Path
from sklearn.metrics import mean_squared_error

from basemodel import *

warnings.filterwarnings('ignore')

# HÃ m fine-tune mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u má»›i
def finetune_model(base_model_path, new_data_path, output_path, features_list):
    base_model_path = str(base_model_path)
    output_path = str(output_path)
    
    if not os.path.exists(base_model_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file model gá»‘c táº¡i {base_model_path}")
        return

    model = joblib.load(base_model_path)
    print("âœ… ÄÃ£ load xong model gá»‘c.")

    # LOAD METADATA (biáº¿t cáº¥u trÃºc train dÃ¹ng cá»™t nÃ o)
    meta_path = base_model_path.replace('.pkl', '_features.pkl')
    input_cols_old, features_old = joblib.load(meta_path)
    print("âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cáº¥u trÃºc input/output cÅ©.")

    # Gá»i hÃ m chuáº©n bá»‹ dá»¯ liá»‡u tá»« basemodel
    print(f"ğŸ”„ Äang xá»­ lÃ½ dá»¯ liá»‡u má»›i tá»«: {new_data_path}")
    df_ft, _ = prepare_time_series_data(new_data_path, features_list, lags=[1, 4])
    if df_ft is None or len(df_ft) == 0:
        print("âš ï¸ Dá»¯ liá»‡u fine-tune trá»‘ng hoáº·c khÃ´ng Ä‘á»§ Ä‘á»ƒ táº¡o lag. Há»§y bá».")
        return

    X_new = df_ft[input_cols_old]
    y_new = df_ft[features_list]

    print(f"ğŸ“Š KÃ­ch thÆ°á»›c dá»¯ liá»‡u Fine-tune: {len(X_new)} máº«u")

    # Fine-tune tá»«ng model con trong MultiOutputRegressor
    for i, estimator in enumerate(model.estimators_):
        target_name = features_list[i]
        old_booster = estimator.get_booster()
        
        estimator.set_params(learning_rate=0.005) 
        
        # gb_model=old_booster Ä‘á»ƒ tiáº¿p tá»¥c tá»« model cÅ©
        estimator.fit(X_new, y_new.iloc[:, i], xgb_model=old_booster)
        
    # ÄÃ¡nh giÃ¡ rmse
    print("\nğŸ“Š Káº¾T QUáº¢ SAU KHI FINE-TUNE (TRÃŠN Táº¬P Dá»® LIá»†U Má»šI):")
    print("-" * 50)
    y_pred = model.predict(X_new)
    rmse = np.sqrt(mean_squared_error(y_new, y_pred, multioutput='raw_values'))
    
    for i, col_name in enumerate(features_list):
        print(f"   ğŸ”¹ {col_name:<15} RMSE: {rmse[i]:.4f}")
    
    print("-" * 50)
    print(f"ğŸ‘‰ RMSE trung bÃ¬nh: {np.mean(rmse):.4f}")

    # LÆ°u model fine-tune
    joblib.dump(model, output_path)
    joblib.dump((input_cols_old, features_list), output_path.replace('.pkl', '_features.pkl'))
    
    print(f"\nğŸ‰ ÄÃ£ lÆ°u model Fine-tune táº¡i: {output_path}")


if __name__ == "__main__":
    BASE_DIR = Path(__file__).resolve().parent
    PROJECT_DIR = BASE_DIR.parent
    
    MODEL_DIR = PROJECT_DIR / "model" / "output"
    BASE_COBIA_MODEL = MODEL_DIR / "hk_cobia_forecast_model.pkl"
    
    # ÄÆ°á»ng dáº«n Ä‘áº¿n dá»¯ liá»‡u cáº§n Ä‘Æ°á»£c fine-tune
    NEW_DATA_PATH = PROJECT_DIR / "data" / "data_quang_ninh" / "qn_env_clean_ready.csv"
    
    # ÄÆ°á»ng dáº«n lÆ°u model má»›i
    OUTPUT_FINETUNE = MODEL_DIR / "hk_cobia_finetuned.pkl"

    
    # Cháº¡y Fine-tune cho CÃ GIÃ’ (sá»­a cÃ¡i nÃ y Ä‘á»ƒ cháº¡y láº¡i cho HÃ€U)
    finetune_model(
        base_model_path = BASE_COBIA_MODEL,
        new_data_path = NEW_DATA_PATH,
        output_path = OUTPUT_FINETUNE,
        features_list = COBIA_FEATURES
    )